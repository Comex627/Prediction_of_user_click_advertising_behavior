{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,BatchNormalization,Dropout\n",
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "sns.set(font_scale=1)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "epochs = 3\n",
    "batch_size = 1024\n",
    "classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "label = pd.read_csv('train_label.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sub = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source = train.merge(label,on='ID',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'] = -1\n",
    "train = train.merge(label,on='ID',how='left')\n",
    "data = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_fe(df):\n",
    "    df['day'] = df.date.apply(lambda x:int(x[8:10]))\n",
    "\n",
    "    df['hour'] = df.date.apply(lambda x:int(x[11:13]))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对时间分箱\n",
    "def getSeg(x):\n",
    "    if x >=0 and x<= 6:\n",
    "        return 1\n",
    "    elif x>=7 and x<=12:\n",
    "        return 2\n",
    "    elif x>=13 and x<=18:\n",
    "        return 3\n",
    "    elif x>=19 and x<=23:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hourDF = train_df.groupby(['hour', 'label'])['hour'].count().unstack('label').fillna(0)\n",
    "# hourDF[[0,1]].plot(kind='bar', stacked=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count统计特征\n",
    "cross_feature = []\n",
    "def get_cross_fe(df):\n",
    "    first_feature = ['B1', 'B2', 'B3']\n",
    "    second_feature = ['C1','C2','C3','D1','D2']\n",
    "    for feat_1 in first_feature:\n",
    "        for feat_2 in second_feature:\n",
    "            col_name = \"cross_\" + feat_1 + \"_and_\" + feat_2\n",
    "            cross_feature.append(col_name)\n",
    "            df[col_name] = df[feat_1].astype(str).values + '_' + df[feat_2].astype(str).values\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_time_fe(data)\n",
    "# data['hour_seg'] = data['hour'].apply(lambda x: getSeg(x))\n",
    "data = get_cross_fe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labelencoder()\n",
    "cate_feature = ['A1','A2','A3','B1','B2','B3','C1','C2','C3','E2','E3','E5','E7','E9','E10','E13','E16','E17','E19','E21','E22']\n",
    "cross_feature = cross_feature[:15]\n",
    "cate_features = cate_feature+cross_feature\n",
    "for item in cate_features:\n",
    "    data[item] = LabelEncoder().fit_transform(data[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_count(data, features=[]):\n",
    "    new_feature = 'count'\n",
    "    for i in features:\n",
    "        new_feature += '_' + i\n",
    "    try:\n",
    "        del data[new_feature]\n",
    "    except:\n",
    "        pass\n",
    "    temp = data.groupby(features).size().reset_index().rename(columns={0: new_feature})\n",
    "    data = data.merge(temp, 'left', on=features)\n",
    "    return data\n",
    "\n",
    "for i in cross_feature:\n",
    "    n = data[i].nunique()\n",
    "    if n > 5:\n",
    "        data = feature_count(data, [i])\n",
    "    else:\n",
    "        print(i, ':', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin ratio clcik...\n",
      "The end\n"
     ]
    }
   ],
   "source": [
    "#ratio：类别偏好的ratio比例特征\n",
    "label_feature =[ 'A2', 'A3']\n",
    "data_temp = data[label_feature]\n",
    "df_feature = pd.DataFrame()\n",
    "data_temp['cnt'] = 1\n",
    "print('Begin ratio clcik...')\n",
    "col_type = label_feature.copy()\n",
    "n = len(col_type)\n",
    "for i in range(n):\n",
    "    col_name = \"ratio_click_of_\" + col_type[i]\n",
    "    df_feature[col_name] = (\n",
    "                    data_temp[col_type[i]].map(data_temp[col_type[i]].value_counts()) / len(data) * 100).astype(int)            \n",
    "data = pd.concat([data, df_feature], axis=1)\n",
    "print('The end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data[data['label'] != -1]\n",
    "test_df = data[data['label'] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.stripplot(train_df[\"label\"],train_df[\"A3\"],jitter=True,order=None)\n",
    "# plt.title(\"click Vs creative_height\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 删除不需要的字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get train feature\n",
    "del_feature = ['ID','day','date','label','D2']+cross_feature\n",
    "features = [i for i in train_df.columns if i not in del_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df[features]\n",
    "train_y = train_df['label'].values\n",
    "test = test_df[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalized\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(train_x)\n",
    "test_X = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple mlp model\n",
    "K.clear_session()\n",
    "def MLP(dropout_rate=0.25, activation='relu'):\n",
    "    start_neurons = 512\n",
    "    model = Sequential()\n",
    "    model.add(Dense(start_neurons, input_dim=train_X.shape[1], activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(start_neurons // 2, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(start_neurons // 4, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(start_neurons // 8, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate / 2))\n",
    "\n",
    "    model.add(Dense(classes, activation='sigmoid'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_acc(history, fold):\n",
    "    plt.plot(history.history['loss'][1:])\n",
    "    plt.plot(history.history['val_loss'][1:])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('val_loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'Validation'], loc='upper left')\n",
    "    plt.savefig('../../result/model_loss' + str(fold) + '.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['acc'][1:])\n",
    "    plt.plot(history.history['val_acc'][1:])\n",
    "    plt.title('model Accuracy')\n",
    "    plt.ylabel('val_acc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'Validation'], loc='upper left')\n",
    "#     plt.savefig('../../result/model_accuracy' + str(fold) + '.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五折交叉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=1\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=2048)\n",
    "NN_predictions = np.zeros((test_X.shape[0], classes))\n",
    "oof_preds = np.zeros((train_X.shape[0], classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 50   ## How many steps to stop\n",
    "call_ES = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=patience, verbose=1,\n",
    "                                        mode='auto', baseline=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/3\n",
      "48000/48000 [==============================] - ETA: 4:32 - loss: 0.9080 - acc: 0.540 - ETA: 1:27 - loss: 0.8866 - acc: 0.526 - ETA: 50s - loss: 0.8677 - acc: 0.529 - ETA: 34s - loss: 0.8548 - acc: 0.53 - ETA: 25s - loss: 0.8458 - acc: 0.53 - ETA: 20s - loss: 0.8350 - acc: 0.53 - ETA: 16s - loss: 0.8230 - acc: 0.54 - ETA: 13s - loss: 0.8116 - acc: 0.54 - ETA: 11s - loss: 0.8046 - acc: 0.54 - ETA: 9s - loss: 0.7939 - acc: 0.5535 - ETA: 8s - loss: 0.7891 - acc: 0.557 - ETA: 6s - loss: 0.7820 - acc: 0.560 - ETA: 5s - loss: 0.7764 - acc: 0.564 - ETA: 4s - loss: 0.7706 - acc: 0.567 - ETA: 4s - loss: 0.7648 - acc: 0.571 - ETA: 3s - loss: 0.7598 - acc: 0.575 - ETA: 2s - loss: 0.7550 - acc: 0.579 - ETA: 2s - loss: 0.7497 - acc: 0.583 - ETA: 1s - loss: 0.7455 - acc: 0.585 - ETA: 1s - loss: 0.7405 - acc: 0.589 - ETA: 1s - loss: 0.7356 - acc: 0.592 - ETA: 0s - loss: 0.7314 - acc: 0.596 - ETA: 0s - loss: 0.7276 - acc: 0.598 - 7s 154us/step - loss: 0.7241 - acc: 0.6021 - val_loss: 0.5746 - val_acc: 0.7927\n",
      "Epoch 2/3\n",
      "48000/48000 [==============================] - ETA: 1s - loss: 0.6336 - acc: 0.684 - ETA: 1s - loss: 0.6233 - acc: 0.691 - ETA: 1s - loss: 0.6238 - acc: 0.697 - ETA: 1s - loss: 0.6215 - acc: 0.700 - ETA: 1s - loss: 0.6160 - acc: 0.707 - ETA: 0s - loss: 0.6139 - acc: 0.711 - ETA: 0s - loss: 0.6126 - acc: 0.714 - ETA: 0s - loss: 0.6101 - acc: 0.718 - ETA: 0s - loss: 0.6091 - acc: 0.720 - ETA: 0s - loss: 0.6063 - acc: 0.723 - ETA: 0s - loss: 0.6047 - acc: 0.725 - ETA: 0s - loss: 0.6028 - acc: 0.728 - ETA: 0s - loss: 0.6016 - acc: 0.729 - ETA: 0s - loss: 0.5995 - acc: 0.732 - ETA: 0s - loss: 0.5980 - acc: 0.734 - ETA: 0s - loss: 0.5961 - acc: 0.737 - ETA: 0s - loss: 0.5943 - acc: 0.739 - ETA: 0s - loss: 0.5917 - acc: 0.741 - ETA: 0s - loss: 0.5908 - acc: 0.743 - ETA: 0s - loss: 0.5893 - acc: 0.744 - ETA: 0s - loss: 0.5882 - acc: 0.746 - ETA: 0s - loss: 0.5870 - acc: 0.747 - ETA: 0s - loss: 0.5850 - acc: 0.749 - 1s 28us/step - loss: 0.5835 - acc: 0.7509 - val_loss: 0.5012 - val_acc: 0.8293\n",
      "Epoch 3/3\n",
      "48000/48000 [==============================] - ETA: 1s - loss: 0.5469 - acc: 0.785 - ETA: 1s - loss: 0.5424 - acc: 0.794 - ETA: 1s - loss: 0.5429 - acc: 0.793 - ETA: 1s - loss: 0.5373 - acc: 0.796 - ETA: 1s - loss: 0.5374 - acc: 0.796 - ETA: 0s - loss: 0.5371 - acc: 0.795 - ETA: 0s - loss: 0.5405 - acc: 0.794 - ETA: 0s - loss: 0.5393 - acc: 0.794 - ETA: 0s - loss: 0.5390 - acc: 0.796 - ETA: 0s - loss: 0.5356 - acc: 0.799 - ETA: 0s - loss: 0.5324 - acc: 0.801 - ETA: 0s - loss: 0.5306 - acc: 0.802 - ETA: 0s - loss: 0.5299 - acc: 0.801 - ETA: 0s - loss: 0.5285 - acc: 0.803 - ETA: 0s - loss: 0.5273 - acc: 0.803 - ETA: 0s - loss: 0.5258 - acc: 0.804 - ETA: 0s - loss: 0.5263 - acc: 0.804 - ETA: 0s - loss: 0.5257 - acc: 0.804 - ETA: 0s - loss: 0.5240 - acc: 0.805 - ETA: 0s - loss: 0.5228 - acc: 0.805 - ETA: 0s - loss: 0.5213 - acc: 0.806 - ETA: 0s - loss: 0.5201 - acc: 0.807 - ETA: 0s - loss: 0.5191 - acc: 0.807 - 1s 28us/step - loss: 0.5183 - acc: 0.8079 - val_loss: 0.4579 - val_acc: 0.8306\n",
      "fold 2\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/3\n",
      "48000/48000 [==============================] - ETA: 1:11 - loss: 0.9317 - acc: 0.509 - ETA: 23s - loss: 0.8949 - acc: 0.514 - ETA: 13s - loss: 0.8778 - acc: 0.52 - ETA: 9s - loss: 0.8764 - acc: 0.5240 - ETA: 7s - loss: 0.8679 - acc: 0.525 - ETA: 5s - loss: 0.8559 - acc: 0.530 - ETA: 4s - loss: 0.8416 - acc: 0.537 - ETA: 4s - loss: 0.8330 - acc: 0.538 - ETA: 3s - loss: 0.8276 - acc: 0.541 - ETA: 3s - loss: 0.8200 - acc: 0.542 - ETA: 2s - loss: 0.8134 - acc: 0.546 - ETA: 2s - loss: 0.8063 - acc: 0.550 - ETA: 1s - loss: 0.7994 - acc: 0.553 - ETA: 1s - loss: 0.7927 - acc: 0.557 - ETA: 1s - loss: 0.7862 - acc: 0.561 - ETA: 1s - loss: 0.7796 - acc: 0.565 - ETA: 1s - loss: 0.7749 - acc: 0.568 - ETA: 0s - loss: 0.7695 - acc: 0.571 - ETA: 0s - loss: 0.7645 - acc: 0.574 - ETA: 0s - loss: 0.7604 - acc: 0.578 - ETA: 0s - loss: 0.7551 - acc: 0.581 - ETA: 0s - loss: 0.7502 - acc: 0.585 - ETA: 0s - loss: 0.7457 - acc: 0.589 - 3s 63us/step - loss: 0.7411 - acc: 0.5935 - val_loss: 0.5726 - val_acc: 0.7936\n",
      "Epoch 2/3\n",
      "48000/48000 [==============================] - ETA: 1s - loss: 0.6273 - acc: 0.669 - ETA: 1s - loss: 0.6223 - acc: 0.678 - ETA: 1s - loss: 0.6242 - acc: 0.686 - ETA: 1s - loss: 0.6243 - acc: 0.687 - ETA: 1s - loss: 0.6218 - acc: 0.692 - ETA: 0s - loss: 0.6197 - acc: 0.693 - ETA: 0s - loss: 0.6183 - acc: 0.695 - ETA: 0s - loss: 0.6177 - acc: 0.699 - ETA: 0s - loss: 0.6155 - acc: 0.702 - ETA: 0s - loss: 0.6126 - acc: 0.706 - ETA: 0s - loss: 0.6107 - acc: 0.708 - ETA: 0s - loss: 0.6104 - acc: 0.710 - ETA: 0s - loss: 0.6086 - acc: 0.712 - ETA: 0s - loss: 0.6067 - acc: 0.714 - ETA: 0s - loss: 0.6030 - acc: 0.718 - ETA: 0s - loss: 0.6012 - acc: 0.721 - ETA: 0s - loss: 0.5987 - acc: 0.724 - ETA: 0s - loss: 0.5962 - acc: 0.726 - ETA: 0s - loss: 0.5950 - acc: 0.729 - ETA: 0s - loss: 0.5930 - acc: 0.731 - ETA: 0s - loss: 0.5911 - acc: 0.733 - ETA: 0s - loss: 0.5902 - acc: 0.735 - ETA: 0s - loss: 0.5883 - acc: 0.738 - 1s 28us/step - loss: 0.5865 - acc: 0.7400 - val_loss: 0.4994 - val_acc: 0.8246\n",
      "Epoch 3/3\n",
      "48000/48000 [==============================] - ETA: 1s - loss: 0.5516 - acc: 0.774 - ETA: 1s - loss: 0.5448 - acc: 0.779 - ETA: 1s - loss: 0.5451 - acc: 0.784 - ETA: 1s - loss: 0.5439 - acc: 0.785 - ETA: 1s - loss: 0.5455 - acc: 0.787 - ETA: 0s - loss: 0.5424 - acc: 0.790 - ETA: 0s - loss: 0.5384 - acc: 0.793 - ETA: 0s - loss: 0.5351 - acc: 0.796 - ETA: 0s - loss: 0.5356 - acc: 0.795 - ETA: 0s - loss: 0.5336 - acc: 0.796 - ETA: 0s - loss: 0.5321 - acc: 0.798 - ETA: 0s - loss: 0.5308 - acc: 0.799 - ETA: 0s - loss: 0.5295 - acc: 0.800 - ETA: 0s - loss: 0.5280 - acc: 0.801 - ETA: 0s - loss: 0.5274 - acc: 0.801 - ETA: 0s - loss: 0.5270 - acc: 0.802 - ETA: 0s - loss: 0.5255 - acc: 0.802 - ETA: 0s - loss: 0.5246 - acc: 0.803 - ETA: 0s - loss: 0.5233 - acc: 0.803 - ETA: 0s - loss: 0.5232 - acc: 0.804 - ETA: 0s - loss: 0.5218 - acc: 0.805 - ETA: 0s - loss: 0.5216 - acc: 0.805 - ETA: 0s - loss: 0.5214 - acc: 0.805 - 1s 29us/step - loss: 0.5214 - acc: 0.8056 - val_loss: 0.4628 - val_acc: 0.8253\n",
      "fold 3\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/3\n",
      "48000/48000 [==============================] - ETA: 1:17 - loss: 0.9582 - acc: 0.502 - ETA: 25s - loss: 0.9527 - acc: 0.506 - ETA: 15s - loss: 0.9222 - acc: 0.51 - ETA: 10s - loss: 0.8968 - acc: 0.52 - ETA: 8s - loss: 0.8818 - acc: 0.5226 - ETA: 6s - loss: 0.8679 - acc: 0.525 - ETA: 5s - loss: 0.8567 - acc: 0.527 - ETA: 4s - loss: 0.8439 - acc: 0.531 - ETA: 3s - loss: 0.8369 - acc: 0.530 - ETA: 3s - loss: 0.8264 - acc: 0.534 - ETA: 2s - loss: 0.8173 - acc: 0.538 - ETA: 2s - loss: 0.8074 - acc: 0.545 - ETA: 2s - loss: 0.7990 - acc: 0.549 - ETA: 1s - loss: 0.7919 - acc: 0.555 - ETA: 1s - loss: 0.7854 - acc: 0.559 - ETA: 1s - loss: 0.7808 - acc: 0.563 - ETA: 1s - loss: 0.7761 - acc: 0.567 - ETA: 0s - loss: 0.7703 - acc: 0.571 - ETA: 0s - loss: 0.7645 - acc: 0.575 - ETA: 0s - loss: 0.7583 - acc: 0.579 - ETA: 0s - loss: 0.7535 - acc: 0.583 - ETA: 0s - loss: 0.7482 - acc: 0.588 - ETA: 0s - loss: 0.7429 - acc: 0.592 - 3s 66us/step - loss: 0.7392 - acc: 0.5955 - val_loss: 0.5560 - val_acc: 0.8155\n",
      "Epoch 2/3\n",
      "48000/48000 [==============================] - ETA: 1s - loss: 0.6424 - acc: 0.669 - ETA: 1s - loss: 0.6197 - acc: 0.693 - ETA: 1s - loss: 0.6239 - acc: 0.694 - ETA: 1s - loss: 0.6219 - acc: 0.698 - ETA: 1s - loss: 0.6172 - acc: 0.704 - ETA: 0s - loss: 0.6134 - acc: 0.709 - ETA: 0s - loss: 0.6147 - acc: 0.709 - ETA: 0s - loss: 0.6105 - acc: 0.713 - ETA: 0s - loss: 0.6099 - acc: 0.716 - ETA: 0s - loss: 0.6089 - acc: 0.717 - ETA: 0s - loss: 0.6065 - acc: 0.720 - ETA: 0s - loss: 0.6040 - acc: 0.723 - ETA: 0s - loss: 0.6018 - acc: 0.725 - ETA: 0s - loss: 0.5997 - acc: 0.728 - ETA: 0s - loss: 0.5985 - acc: 0.729 - ETA: 0s - loss: 0.5964 - acc: 0.732 - ETA: 0s - loss: 0.5953 - acc: 0.733 - ETA: 0s - loss: 0.5938 - acc: 0.736 - ETA: 0s - loss: 0.5915 - acc: 0.738 - ETA: 0s - loss: 0.5897 - acc: 0.741 - ETA: 0s - loss: 0.5879 - acc: 0.743 - ETA: 0s - loss: 0.5862 - acc: 0.745 - ETA: 0s - loss: 0.5848 - acc: 0.747 - 1s 28us/step - loss: 0.5839 - acc: 0.7492 - val_loss: 0.5095 - val_acc: 0.8267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3\n",
      "48000/48000 [==============================] - ETA: 1s - loss: 0.5221 - acc: 0.810 - ETA: 1s - loss: 0.5314 - acc: 0.801 - ETA: 1s - loss: 0.5313 - acc: 0.800 - ETA: 1s - loss: 0.5313 - acc: 0.803 - ETA: 1s - loss: 0.5297 - acc: 0.806 - ETA: 0s - loss: 0.5305 - acc: 0.804 - ETA: 0s - loss: 0.5335 - acc: 0.801 - ETA: 0s - loss: 0.5317 - acc: 0.802 - ETA: 0s - loss: 0.5305 - acc: 0.803 - ETA: 0s - loss: 0.5286 - acc: 0.804 - ETA: 0s - loss: 0.5261 - acc: 0.805 - ETA: 0s - loss: 0.5261 - acc: 0.806 - ETA: 0s - loss: 0.5254 - acc: 0.806 - ETA: 0s - loss: 0.5261 - acc: 0.806 - ETA: 0s - loss: 0.5248 - acc: 0.806 - ETA: 0s - loss: 0.5243 - acc: 0.806 - ETA: 0s - loss: 0.5222 - acc: 0.807 - ETA: 0s - loss: 0.5216 - acc: 0.807 - ETA: 0s - loss: 0.5210 - acc: 0.807 - ETA: 0s - loss: 0.5204 - acc: 0.808 - ETA: 0s - loss: 0.5204 - acc: 0.808 - ETA: 0s - loss: 0.5187 - acc: 0.809 - ETA: 0s - loss: 0.5181 - acc: 0.809 - 1s 28us/step - loss: 0.5174 - acc: 0.8098 - val_loss: 0.4588 - val_acc: 0.8308\n",
      "fold 4\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/3\n",
      "48000/48000 [==============================] - ETA: 1:23 - loss: 0.9256 - acc: 0.515 - ETA: 27s - loss: 0.9059 - acc: 0.516 - ETA: 16s - loss: 0.8859 - acc: 0.51 - ETA: 11s - loss: 0.8805 - acc: 0.52 - ETA: 8s - loss: 0.8697 - acc: 0.5245 - ETA: 6s - loss: 0.8546 - acc: 0.529 - ETA: 5s - loss: 0.8449 - acc: 0.530 - ETA: 4s - loss: 0.8342 - acc: 0.536 - ETA: 3s - loss: 0.8223 - acc: 0.541 - ETA: 3s - loss: 0.8150 - acc: 0.545 - ETA: 2s - loss: 0.8078 - acc: 0.546 - ETA: 2s - loss: 0.8008 - acc: 0.549 - ETA: 2s - loss: 0.7933 - acc: 0.553 - ETA: 1s - loss: 0.7860 - acc: 0.558 - ETA: 1s - loss: 0.7801 - acc: 0.562 - ETA: 1s - loss: 0.7748 - acc: 0.565 - ETA: 1s - loss: 0.7704 - acc: 0.569 - ETA: 0s - loss: 0.7642 - acc: 0.573 - ETA: 0s - loss: 0.7583 - acc: 0.577 - ETA: 0s - loss: 0.7523 - acc: 0.581 - ETA: 0s - loss: 0.7471 - acc: 0.585 - ETA: 0s - loss: 0.7432 - acc: 0.587 - ETA: 0s - loss: 0.7386 - acc: 0.591 - 3s 69us/step - loss: 0.7350 - acc: 0.5944 - val_loss: 0.5686 - val_acc: 0.8090\n",
      "Epoch 2/3\n",
      "48000/48000 [==============================] - ETA: 1s - loss: 0.6342 - acc: 0.676 - ETA: 1s - loss: 0.6376 - acc: 0.684 - ETA: 1s - loss: 0.6341 - acc: 0.684 - ETA: 1s - loss: 0.6312 - acc: 0.685 - ETA: 1s - loss: 0.6312 - acc: 0.686 - ETA: 1s - loss: 0.6294 - acc: 0.691 - ETA: 0s - loss: 0.6257 - acc: 0.694 - ETA: 0s - loss: 0.6211 - acc: 0.697 - ETA: 0s - loss: 0.6203 - acc: 0.699 - ETA: 0s - loss: 0.6167 - acc: 0.702 - ETA: 0s - loss: 0.6157 - acc: 0.705 - ETA: 0s - loss: 0.6128 - acc: 0.708 - ETA: 0s - loss: 0.6112 - acc: 0.710 - ETA: 0s - loss: 0.6086 - acc: 0.713 - ETA: 0s - loss: 0.6064 - acc: 0.716 - ETA: 0s - loss: 0.6042 - acc: 0.719 - ETA: 0s - loss: 0.6027 - acc: 0.721 - ETA: 0s - loss: 0.6014 - acc: 0.723 - ETA: 0s - loss: 0.5990 - acc: 0.726 - ETA: 0s - loss: 0.5976 - acc: 0.728 - ETA: 0s - loss: 0.5953 - acc: 0.730 - ETA: 0s - loss: 0.5943 - acc: 0.732 - ETA: 0s - loss: 0.5929 - acc: 0.734 - 1s 28us/step - loss: 0.5908 - acc: 0.7365 - val_loss: 0.5070 - val_acc: 0.8227\n",
      "Epoch 3/3\n",
      "48000/48000 [==============================] - ETA: 1s - loss: 0.5344 - acc: 0.790 - ETA: 1s - loss: 0.5430 - acc: 0.788 - ETA: 1s - loss: 0.5495 - acc: 0.784 - ETA: 1s - loss: 0.5455 - acc: 0.784 - ETA: 1s - loss: 0.5440 - acc: 0.784 - ETA: 0s - loss: 0.5443 - acc: 0.785 - ETA: 0s - loss: 0.5431 - acc: 0.786 - ETA: 0s - loss: 0.5396 - acc: 0.788 - ETA: 0s - loss: 0.5375 - acc: 0.789 - ETA: 0s - loss: 0.5355 - acc: 0.790 - ETA: 0s - loss: 0.5334 - acc: 0.792 - ETA: 0s - loss: 0.5315 - acc: 0.794 - ETA: 0s - loss: 0.5312 - acc: 0.794 - ETA: 0s - loss: 0.5308 - acc: 0.794 - ETA: 0s - loss: 0.5294 - acc: 0.795 - ETA: 0s - loss: 0.5305 - acc: 0.794 - ETA: 0s - loss: 0.5292 - acc: 0.795 - ETA: 0s - loss: 0.5287 - acc: 0.796 - ETA: 0s - loss: 0.5269 - acc: 0.797 - ETA: 0s - loss: 0.5249 - acc: 0.798 - ETA: 0s - loss: 0.5239 - acc: 0.799 - ETA: 0s - loss: 0.5237 - acc: 0.800 - ETA: 0s - loss: 0.5229 - acc: 0.800 - 1s 28us/step - loss: 0.5222 - acc: 0.8013 - val_loss: 0.4701 - val_acc: 0.8225\n",
      "fold 5\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/3\n",
      "48000/48000 [==============================] - ETA: 1:39 - loss: 0.9636 - acc: 0.475 - ETA: 32s - loss: 0.9281 - acc: 0.488 - ETA: 19s - loss: 0.9056 - acc: 0.50 - ETA: 13s - loss: 0.8900 - acc: 0.50 - ETA: 10s - loss: 0.8782 - acc: 0.50 - ETA: 8s - loss: 0.8726 - acc: 0.5087 - ETA: 6s - loss: 0.8600 - acc: 0.515 - ETA: 5s - loss: 0.8525 - acc: 0.518 - ETA: 4s - loss: 0.8452 - acc: 0.522 - ETA: 3s - loss: 0.8344 - acc: 0.524 - ETA: 3s - loss: 0.8259 - acc: 0.527 - ETA: 2s - loss: 0.8170 - acc: 0.531 - ETA: 2s - loss: 0.8095 - acc: 0.534 - ETA: 2s - loss: 0.8018 - acc: 0.537 - ETA: 1s - loss: 0.7950 - acc: 0.542 - ETA: 1s - loss: 0.7885 - acc: 0.545 - ETA: 1s - loss: 0.7832 - acc: 0.549 - ETA: 1s - loss: 0.7770 - acc: 0.553 - ETA: 0s - loss: 0.7718 - acc: 0.557 - ETA: 0s - loss: 0.7666 - acc: 0.561 - ETA: 0s - loss: 0.7618 - acc: 0.565 - ETA: 0s - loss: 0.7568 - acc: 0.568 - ETA: 0s - loss: 0.7523 - acc: 0.572 - 4s 78us/step - loss: 0.7478 - acc: 0.5762 - val_loss: 0.7021 - val_acc: 0.6325\n",
      "Epoch 2/3\n",
      "48000/48000 [==============================] - ETA: 1s - loss: 0.6695 - acc: 0.649 - ETA: 1s - loss: 0.6456 - acc: 0.667 - ETA: 1s - loss: 0.6320 - acc: 0.682 - ETA: 1s - loss: 0.6264 - acc: 0.686 - ETA: 1s - loss: 0.6271 - acc: 0.690 - ETA: 0s - loss: 0.6226 - acc: 0.697 - ETA: 0s - loss: 0.6232 - acc: 0.698 - ETA: 0s - loss: 0.6183 - acc: 0.703 - ETA: 0s - loss: 0.6144 - acc: 0.708 - ETA: 0s - loss: 0.6107 - acc: 0.712 - ETA: 0s - loss: 0.6090 - acc: 0.714 - ETA: 0s - loss: 0.6071 - acc: 0.715 - ETA: 0s - loss: 0.6053 - acc: 0.718 - ETA: 0s - loss: 0.6040 - acc: 0.719 - ETA: 0s - loss: 0.6022 - acc: 0.722 - ETA: 0s - loss: 0.6000 - acc: 0.724 - ETA: 0s - loss: 0.5980 - acc: 0.727 - ETA: 0s - loss: 0.5957 - acc: 0.729 - ETA: 0s - loss: 0.5938 - acc: 0.731 - ETA: 0s - loss: 0.5926 - acc: 0.733 - ETA: 0s - loss: 0.5905 - acc: 0.736 - ETA: 0s - loss: 0.5892 - acc: 0.737 - ETA: 0s - loss: 0.5881 - acc: 0.739 - 1s 28us/step - loss: 0.5863 - acc: 0.7417 - val_loss: 0.5870 - val_acc: 0.8175\n",
      "Epoch 3/3\n",
      "48000/48000 [==============================] - ETA: 1s - loss: 0.5678 - acc: 0.782 - ETA: 1s - loss: 0.5516 - acc: 0.793 - ETA: 1s - loss: 0.5504 - acc: 0.792 - ETA: 1s - loss: 0.5518 - acc: 0.790 - ETA: 1s - loss: 0.5466 - acc: 0.794 - ETA: 0s - loss: 0.5432 - acc: 0.797 - ETA: 0s - loss: 0.5376 - acc: 0.800 - ETA: 0s - loss: 0.5364 - acc: 0.800 - ETA: 0s - loss: 0.5357 - acc: 0.801 - ETA: 0s - loss: 0.5335 - acc: 0.803 - ETA: 0s - loss: 0.5322 - acc: 0.803 - ETA: 0s - loss: 0.5308 - acc: 0.804 - ETA: 0s - loss: 0.5293 - acc: 0.804 - ETA: 0s - loss: 0.5282 - acc: 0.805 - ETA: 0s - loss: 0.5273 - acc: 0.805 - ETA: 0s - loss: 0.5265 - acc: 0.805 - ETA: 0s - loss: 0.5264 - acc: 0.805 - ETA: 0s - loss: 0.5250 - acc: 0.805 - ETA: 0s - loss: 0.5245 - acc: 0.805 - ETA: 0s - loss: 0.5246 - acc: 0.805 - ETA: 0s - loss: 0.5237 - acc: 0.805 - ETA: 0s - loss: 0.5227 - acc: 0.806 - ETA: 0s - loss: 0.5211 - acc: 0.807 - 1s 28us/step - loss: 0.5199 - acc: 0.8079 - val_loss: 0.5140 - val_acc: 0.8269\n"
     ]
    }
   ],
   "source": [
    "for fold_, (trn_, val_) in enumerate(folds.split(train_x)):\n",
    "    print(\"fold {}\".format(fold_ + 1))\n",
    "    x_train, y_train = train_X[trn_], train_y[trn_]\n",
    "    x_valid, y_valid = train_X[val_], train_y[val_]\n",
    "\n",
    "\n",
    "    model = MLP(dropout_rate=0.5, activation='relu')\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        validation_data=[x_valid, y_valid],\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[call_ES, ],\n",
    "                        shuffle=True,\n",
    "                        verbose=1)\n",
    "\n",
    "    # plot_loss_acc(history, fold_ + 1)\n",
    "\n",
    "    # # Get predicted probabilities for each class\n",
    "    oof_preds[val_] = model.predict_proba(x_valid, batch_size=batch_size)\n",
    "    NN_predictions += model.predict_proba(test_X, batch_size=batch_size) / folds.n_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# roc评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6248683825016098"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score=roc_auc_score(train_y,oof_preds)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['label'] = NN_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['label'] = sub['label'].apply(lambda x:'%.2f'%x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    49756\n",
       "1    10244\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('C:/Users/DELL/Desktop/sub/'+str('%.6f'%score)+'keras.csv',encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature_importance展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_importance_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-45f83a36e4c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## plot feature importance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfeature_importance_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Feature\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"importance\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Feature\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"importance\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mbest_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_importance_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_importance_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFeature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'importance'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m sns.barplot(y=\"Feature\",\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_importance_df' is not defined"
     ]
    }
   ],
   "source": [
    "## plot feature importance\n",
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]].groupby(\"Feature\").mean().sort_values(by=\"importance\", ascending=False)[:].index)\n",
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)].sort_values(by='importance',ascending=False)\n",
    "plt.figure(figsize=(8, 14))\n",
    "sns.barplot(y=\"Feature\",\n",
    "            x=\"importance\",\n",
    "            data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_more_fe(df):\n",
    "#     uniq_list = ['C1','E14','A2','E1','E27']\n",
    "#     for i in uniq_list:\n",
    "#         tmp = df.groupby(['hour'])[i].nunique().reset_index(name = i+'_h_uniq')\n",
    "#         df  = df.merge(tmp, on=['hour'], how='left')\n",
    "#         tmp1 = df.groupby(['date'])[i].nunique().reset_index(name = i+'_d_uniq')\n",
    "#         df  = df.merge(tmp1, on=['date'], how='left')\n",
    "#     dev_list = ['D1','D2']\n",
    "#     for i in dev_list:\n",
    "#         tmp = df.groupby(['C1'])[i].nunique().reset_index(name = i+'_C1_de_uniq')\n",
    "#         df  = df.merge(tmp, on=['C1'], how='left')\n",
    "#         tmp1 = df.groupby(['C2'])[i].nunique().reset_index(name = i+'_C2_de_uniq')\n",
    "#         df  = df.merge(tmp1, on=['C2'], how='left')\n",
    "#         tmp2 = df.groupby(['C3'])[i].nunique().reset_index(name = i+'_C3_de_uniq')\n",
    "#         df  = df.merge(tmp2, on=['C3'], how='left')\n",
    "#     return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
